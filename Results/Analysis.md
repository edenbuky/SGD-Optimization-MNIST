# Results & Analysis ğŸ“Š

## **1.Analysis for Hinge Loss Optimization**
Working with the MNIST data set. We aim to optimize the Hinge Loss with L2-regularization using Stochastic Gradient Descent (SGD). The regularized loss function is defined as:
ğ“›(ğ°,ğ±,ğ‘¦) = ğ¶âˆ™ğ¦ğšğ±\{ğŸ¢,ğŸ·-ğ‘¦âŸ¨ğ±,ğ°âŸ©\}+ğŸ¢.5âˆ¥ğ°âˆ¥Â²
We initialize ğ°â‚ = ğŸ¢, and at each iteration ğ‘¡ = ğŸ·,... we sample ğ‘– uniformly, and if ğ‘¦áµ¢ğ°â‚œâˆ™ğ±áµ¢ < ğŸ·, we update: ğ°â‚œâ‚Šâ‚ = (ğŸ·-ğœ‚â‚œ)ğ°â‚œ + ğœ‚â‚œğ¶ğ‘¦áµ¢ğ±áµ¢.
And  ğ°â‚œâ‚Šâ‚ =(ğŸ·-ğœ‚â‚œ)ğ°â‚œ otherwise, where ğœ‚â‚œ = ğœ‚â‚€/ğ‘¡ and ğœ‚â‚€ is a constant learning rate.

## **(a)Learning Rate Optimization**
### **Problem Statement**
Train the classifier on the training set. Use cross validation on the validation set to find the best ğœ‚â‚€, assuming 'ğ‘‡ = ğŸ·ğŸ¢ğŸ¢ğŸ¢' and ğ¶ = ğŸ·. For each possible ğœ‚â‚€ 
(for example, you can search on the log scale ğœ‚â‚€= ğŸ·ğŸ¢â»âµ,ğŸ·ğŸ¢â»â´,...,ğŸ·ğŸ¢â´,ğŸ·ğŸ¢âµ and increase resolution if needed), assess the performance of ğœ‚â‚€ by averaging the accuracy on the validation set across 10 runs.
Plot to average accuracy on the validation set as a function of ğœ‚â‚€.
### **Solution**
I conducted a run with learning rates 
ğœ‚â‚€âˆˆ\{ğŸ·ğŸ¢â»âµ,ğŸ·ğŸ¢â»â´,...,ğŸ·ğŸ¢â´,ğŸ·ğŸ¢âµ\} 
The best result was obtained for ğœ‚â‚€ = ğŸ·, with an **accuracy of 97.6%**.

<img width="425" alt="Screenshot 2025-02-11 at 16 28 33" src="https://github.com/user-attachments/assets/40264713-0aa9-48c6-8d9d-0a7bfaa88011" />

To refine the learning rate, we conducted a second run in the range ğœ‚â‚€âˆˆ\{ğŸ¢,ğŸ¢.ğŸ·,ğŸ¢.ğŸ¸...,ğŸ·.ğŸ¿,ğŸ¸} 
The best result was obtained for ğœ‚â‚€ = ğŸ¢.ğŸ½ with an **accuracy of 98.16%**.

<img width="447" alt="Screenshot 2025-02-11 at 16 28 22" src="https://github.com/user-attachments/assets/a0e51d38-76de-442c-8e38-e145a48179c2" />

---

## **(b) Cross-Validation for Regularization Parameter ğ¶**
### **Problem Statement**
Now, cross-validate on the validation set to find the best ğ¶ given the best ğœ‚â‚€ found in section (a). For each possible 
ğ¶ (again, you can search on the log scale as in section (a)), average the accuracy on the validation set across 10 runs. Plot the average accuracy on the validation set as a function of ğ¶.

### **Solution**
As shown in the initial search over the range ğ¶âˆˆ\{ğŸ·ğŸ¢â»â´,...,ğŸ·ğŸ¢â´\}, the result suggests that ğ¶ = ğŸ¢.ğŸ¢ğŸ¢ğŸ¢ğŸ·
yields the best **accuracy of 98.54%**.
In a refined search over the range ğ¶âˆˆ\{ğŸ¢,ğŸ¢.ğŸ¢ğŸ»,...ğŸ¢.ğŸ»\}, ğ¶ = ğŸ¢.ğŸ¸ achieves an accuracy of 98.29%,
which is lower than the performance at ğ¶ = ğŸ¢.ğŸ¢ğŸ¢ğŸ¢ğŸ·. Thus, we set the final regularization parameter to  ğ¶ = ğŸ¢.ğŸ¢ğŸ¢ğŸ¢ğŸ·.
<img width="592" alt="Screenshot 2025-02-11 at 16 28 48" src="https://github.com/user-attachments/assets/68ed3214-bd90-4e36-a85a-3579cec870e6" />

---
## **(c) Visualizing the Weight Vector ğ°**
### **Problem Statement**
Using the best ğ¶, ğœ‚â‚€ you found, train the classifier, but for ğ‘‡ = ğŸ¸ğŸ¢ğŸ¢ğŸ¢ğŸ¢.
Show the resulting ğ° as an image, e.g. using the following ğš–ğšŠğšğš™ğš•ğš˜ğšğš•ğš’ğš‹.ğš™ğš¢ğš™ğš•ğš˜ğš function: ğš’ğš–ğšœğš‘ğš˜ğš (ğš›ğšğšœğš‘ğšŠğš™ğš(ğš’ğš–ğšŠğšğš,(ğŸ¸ğŸ¾,ğŸ¸ğŸ¾)), ğš’ğš—ğšğšğš›ğš™ğš˜ğš•ğšŠğšğš’ğš˜ğš—='ğš—ğšğšŠğš›ğšğšœğš').
Give an intuitive interpretation of the image you obtain.
### **Solution**
<img width="371" alt="Screenshot 2025-02-11 at 16 29 04" src="https://github.com/user-attachments/assets/d9205797-6113-4b30-b06b-0bf2b1b03379" />
In this image, each pixel corresponds proportionally to the associated weight value.
â—¦ **Bright regions** indicate **positive weights**, where higher pixel values increase the likelihood of the classifier predicting the positive class label.
â—¦ **Dark regions** indicate **negative weights**, where higher pixel values decrease the likelihood of a positive prediction.
The dark surrounding areas suggest that high pixel values in these regions are less likely to be associated with the positive class, indicating the presence of the digit "0."
In contrast, bright areas in the dark regions likely indicate that it is not a "0."
Meanwhile, the digit "8" appears to occupy the center of the image with positive values, showing where high pixel values are likely to predict "8" accurately.

---

## **(d) Best Accuracy **
What is the accuracy of the best classifier on the test set?
### **Solution:**
The classifier's best accuracy with ğ¶ = ğŸ¢.ğŸ¢ğŸ¢ğŸ¢ğŸ·, ğœ‚â‚€ = ğŸ¢.ğŸ½ and  ğ‘‡ = ğŸ¸ğŸ¢ğŸ¢ğŸ¢ğŸ¢ is **99.28%**

---

## **1.Analysis for Hinge Loss Optimization**
In this exercise we will optimize the log loss defined as follows:
<img width="432" alt="Screenshot 2025-02-11 at 18 07 07" src="https://github.com/user-attachments/assets/7af33d06-5dae-40a4-a091-5f7239cd7d22" />
(In the lecture you defined the loss with logâ‚‚(â‹…), but for optimization purposes the logarithm base doesnâ€™t matter).
Derive the gradient update for this case, and implement the appropriate SGD function.
In your computations, it is recommended to use various built-in functions (ğšœğšŒğš’ğš™ğš¢.ğšœğš™ğšğšŒğš’ğšŠğš•.ğšœğš˜ğšğšğš–ğšŠğš¡ might be helpful) in order to avoid numerical issues which arise from exponentiating very large numbers.
### **(a)**
Train the classifier on the training set using SGD. Use cross-validation on the validation set to find the best ğœ‚â‚€ assuming ğ‘‡ = ğŸ·ğŸ¢ğŸ¢ğŸ¢.
For each possible ğœ‚â‚€ (for example, you can search on the log scale ğœ‚â‚€ = ğŸ·ğŸ¢â»âµ,ğŸ·ğŸ¢â»â´,...,ğŸ·ğŸ¢â´,ğŸ·ğŸ¢âµ and increase resolution if needed), assess the performance of ğœ‚â‚€ by averaging the accuracy on the validation set across 10 runs. Plot the average accuracy on the validation set as a function of ğœ‚â‚€â€‹.
### **Solution:**
As can be seen, ğœ‚â‚€ âˆˆ\{ ğŸ·ğŸ¢â»âµ,ğŸ·ğŸ¢â»â´,...,ğŸ·ğŸ¢â´,ğŸ·ğŸ¢âµ\} gives ğœ‚â‚€ = ğŸ·ğŸ¢â»âµ with an accuracy of **95.52%**.

<img width="398" alt="Screenshot 2025-02-11 at 16 30 40" src="https://github.com/user-attachments/assets/4efec72f-a1f0-4513-befc-60955ea7f10b" />

Therefore, we focus on the range  ğœ‚â‚€ âˆˆ\{ ğŸ·ğŸ¢â»â¶,ğŸ¸âˆ™ğŸ·ğŸ¢â»â¶,...,ğŸ·ğŸ¢â»âµ\} In this range, we found that ğœ‚â‚€ = 3âˆ™ğŸ·ğŸ¢â»â¶ achieves the maximum accuracy of 96.45%.

<img width="516" alt="Screenshot 2025-02-11 at 16 29 20" src="https://github.com/user-attachments/assets/4e170996-1b77-46bd-9f7a-9c8407024d74" />

---
### **(b)**
Using the best ğœ‚â‚€ you found, train the classifier, but for ğ‘‡ = ğŸ¸ğŸ¢ğŸ¢ğŸ¢ğŸ¢.
Show the resulting ğ° as an image. What is the accuracy of the best classifier on the test set?
### **Solution:**

<img width="216" alt="Screenshot 2025-02-11 at 16 32 46" src="https://github.com/user-attachments/assets/53af1081-ed89-4ce5-9a6a-f5d0430fe5f0" />

As before, brighter regions indicate positive weights, where higher pixel values contribute to predicting the positive class, which is "8".
Darker regions indicate negative weights, where higher pixel values in those regions contribute to predicting the negative class, which is "0".
With ğœ‚â‚€ = 3âˆ™ğŸ·ğŸ¢â»â¶, we achieved an accuracy of approximately **97.44%**. This performance level is quite good.

---
### **(c)**
Train the classifier for ğ‘‡ = ğŸ¸ğŸ¢ğŸ¢ğŸ¢ğŸ¢ iterations, and plot the norm of ğ° as a function of the iteration. How does the norm change as SGD progresses? Explain the phenomenon you observe.
### **Solution:**
<img width="278" alt="Screenshot 2025-02-11 at 16 32 55" src="https://github.com/user-attachments/assets/785b5e00-60b0-4316-b23e-150aa1f80e9f" />

As shown in the graph,âˆ¥ğ°âˆ¥ increases sharply at the beginning and then grows more gradually. This occurs because, at the initial stage, ğ° is far from optimal, leading to significant updates that cause the norm to rise rapidly.
As the SGD algorithm progresses, the updates become smaller due to the decreasing learning rate
ğœ‚â‚œ = ğœ‚â‚€/ğ‘¡, and the weight vector converges toward the optimal solution, causing the norm to stabilize. This also indicates that most of the learning happens in the early iterations. After a certain point, the incremental learning becomes minimal, and the norm of ğ° changes insignificantly.
